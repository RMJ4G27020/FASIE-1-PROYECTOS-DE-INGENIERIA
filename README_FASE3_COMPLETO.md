# README - Fase 3: Weight Tokens (Actividades 7-10)

## üìã Tabla de Contenidos
- [Introducci√≥n](#introducci√≥n)
- [Actividad 7: Archivo Posting](#actividad-7-archivo-posting)
- [Actividad 8: Hash Table](#actividad-8-hash-table)
- [Actividad 9: Stop List](#actividad-9-stop-list)
- [Actividad 10: TF-IDF Weight Tokens](#actividad-10-tf-idf-weight-tokens)
- [C√≥mo Ejecutar](#c√≥mo-ejecutar)
- [Resultados y M√©tricas](#resultados-y-m√©tricas)
- [Arquitectura T√©cnica](#arquitectura-t√©cnica)

---

## üéØ Introducci√≥n

La **Fase 3: Weight Tokens** representa la culminaci√≥n del proyecto de procesamiento HTML, implementando t√©cnicas avanzadas de recuperaci√≥n de informaci√≥n y an√°lisis de texto. Esta fase transforma el sistema b√°sico de frecuencias en un motor sofisticado de an√°lisis sem√°ntico.

### Objetivos de la Fase 3:
1. **üóÇÔ∏è Indexaci√≥n Avanzada** - Crear estructuras de posting para b√∫squedas eficientes
2. **‚ö° Optimizaci√≥n de Rendimiento** - Implementar hash tables para acceso r√°pido
3. **üßπ Limpieza Inteligente** - Filtrar t√©rminos irrelevantes autom√°ticamente
4. **üìä An√°lisis Sem√°ntico** - Calcular relevancia real de t√©rminos con TF-IDF

### Progresi√≥n del Proyecto:
```
Fase 1 (Actividades 1-3) ‚Üí Procesamiento B√°sico
Fase 2 (Actividades 4-6) ‚Üí An√°lisis de Frecuencias  
Fase 3 (Actividades 7-10) ‚Üí Weight Tokens & Sem√°ntica ‚ú®
```

---

## üìÇ Actividad 7: Archivo Posting

### üéØ Prop√≥sito
Crear archivos posting que permitan identificar r√°pidamente en qu√© documentos aparece cada token y con qu√© frecuencia, estableciendo la base para un sistema de recuperaci√≥n de informaci√≥n.

### üîß Funcionamiento T√©cnico

La actividad implementa tres componentes principales:

1. **Diccionario Posting** - Resumen de tokens con estad√≠sticas
2. **Lista de Posting** - Mapeo detallado token ‚Üí documentos
3. **An√°lisis Estad√≠stico** - M√©tricas de distribuci√≥n

#### Algoritmo de Procesamiento:
```python
for documento in archivos_html:
    1. Limpiar contenido HTML (remover etiquetas)
    2. Extraer tokens alfab√©ticos (‚â•2 caracteres)
    3. Contar frecuencias por documento
    4. Actualizar √≠ndice inverso: token ‚Üí {documentos}
    5. Acumular estad√≠sticas globales
```

### üìä Estructuras de Datos

```python
# Estructura principal de posting
token_doc_freq = defaultdict(Counter)  # token ‚Üí {doc_id: frecuencia}
token_total_freq = Counter()           # token ‚Üí frecuencia_total

# Ejemplo de datos internos:
# token_doc_freq["government"] = {
#     "doc001": 5,
#     "doc045": 3,
#     "doc123": 8
# }
```

### üìÑ Salidas Generadas

#### 1. **dictionary_posting.txt** - Diccionario con 3 Columnas
```
Token	Repeticiones	#Documentos
--------------------------------------------------
aa	37	8
aaa	24	7
government	459	122
democracy	62	34
information	485	163
medical	284	57
```

**Explicaci√≥n de columnas:**
- **Token**: Palabra extra√≠da del corpus
- **Repeticiones**: Frecuencia total en todos los documentos
- **#Documentos**: N√∫mero de documentos que contienen el token

#### 2. **posting_list.txt** - Lista Detallada de Posting
```
ARCHIVO POSTING - DOCUMENTOS POR TOKEN
============================================================
Total de tokens √∫nicos: 90,831
Total de documentos: 506
============================================================

TOKEN: government
  Frecuencia total: 459
  Aparece en 122 documentos:
    doc123: 8 veces
    doc045: 7 veces
    doc078: 6 veces
    doc234: 5 veces
    ...

TOKEN: democracy
  Frecuencia total: 62
  Aparece en 34 documentos:
    doc087: 4 veces
    doc156: 3 veces
    doc289: 3 veces
    ...
```

#### 3. **posting_stats.txt** - Estad√≠sticas Resumidas
```
ESTAD√çSTICAS DEL ARCHIVO POSTING
==================================================
Total de tokens √∫nicos: 90,831
Total de ocurrencias: 857,723
Total de documentos: 506
Promedio de tokens por documento: 1695.1

TOKEN M√ÅS FRECUENTE:
  'the': 33,472 ocurrencias
  Aparece en 395 documentos

TOKEN M√ÅS DISTRIBUIDO:
  'the': aparece en 395 documentos
  Frecuencia total: 33,472

TOP 10 TOKENS M√ÅS FRECUENTES:
   1. the: 33,472 veces (395 docs)
   2. of: 21,031 veces (392 docs)
   3. and: 17,252 veces (384 docs)
   4. com: 12,152 veces (188 docs)
   5. to: 11,431 veces (383 docs)
```

### ‚ö° Rendimiento
- **Tiempo de ejecuci√≥n**: 1.367 segundos
- **Tokens procesados**: 90,831 √∫nicos
- **Documentos analizados**: 506
- **Archivos generados**: 4 (13.2 MB total)

---

## üî® Actividad 8: Hash Table

### üéØ Prop√≥sito
Implementar una tabla hash personalizada para optimizar las b√∫squedas en el diccionario de tokens, proporcionando acceso r√°pido a la informaci√≥n de cualquier t√©rmino.

### üîß Funcionamiento T√©cnico

#### Caracter√≠sticas de la Implementaci√≥n:

1. **Funci√≥n Hash SHA256** - Distribuci√≥n uniforme de elementos
2. **Encadenamiento Separado** - Manejo de colisiones mediante listas enlazadas
3. **An√°lisis de Rendimiento** - M√©tricas de eficiencia en tiempo real
4. **Casos de Prueba** - Validaci√≥n autom√°tica de funcionalidad

#### Algoritmo de Hash:
```python
def _hash_function(self, key: str) -> int:
    # Usar SHA256 para mejor distribuci√≥n
    hash_bytes = hashlib.sha256(key.encode('utf-8')).digest()
    # Convertir primeros 4 bytes a entero
    hash_int = int.from_bytes(hash_bytes[:4], byteorder='big')
    return hash_int % self.size
```

#### Manejo de Colisiones:
```python
class HashNode:
    def __init__(self, key: str, value: Dict[str, Any]):
        self.key = key
        self.value = value
        self.next: Optional['HashNode'] = None  # Encadenamiento
```

### üìä M√©tricas de Rendimiento

#### Estad√≠sticas de la Tabla Hash:
```
Total de elementos: 90,831
Tama√±o de la tabla: 10,000
Slots ocupados: 9,998
Factor de carga: 9.083
Colisiones: 80,833
Longitud promedio de cadena: 9.08
Longitud m√°xima de cadena: 23
```

#### Resultados de Rendimiento:
```
Tiempo de b√∫squeda: 0.0011 segundos
Velocidad de b√∫squeda: 896,985 b√∫squedas/seg
Ratio de encontrados: 1.000 (100%)
Tiempo de inserci√≥n: 0.0021 segundos
Velocidad de inserci√≥n: 480,778 inserciones/seg
```

### üìÑ Salidas Generadas

#### 1. **hash_table_analysis.txt** - An√°lisis Completo
```
AN√ÅLISIS DE TABLA HASH - ACTIVIDAD 8
============================================================

ESTAD√çSTICAS DE LA TABLA HASH:
----------------------------------------
Total de elementos: 90,831
Tama√±o de la tabla: 10,000
Factor de carga: 9.083
Colisiones: 80,833

COMPARACI√ìN DE FUNCIONES HASH:
----------------------------------------
SHA256:
  Colisiones: 85,831
  Factor de carga: 18.166
  Distribuci√≥n: Excelente

SIMPLE:
  Colisiones: 88,234
  Factor de carga: 18.456
  Distribuci√≥n: Buena
```

#### 2. **hash_table_tests.txt** - Casos de Prueba
```
CASOS DE PRUEBA - TABLA HASH
==================================================

1. PRUEBA DE INSERCI√ìN Y B√öSQUEDA:
   ‚úì 'government': {'repetitions': 459, 'documents': 122}
   ‚úì 'democracy': {'repetitions': 62, 'documents': 34}
   ‚úì 'information': {'repetitions': 485, 'documents': 163}
   
   Resultado: 20/20 pruebas exitosas
   Tasa de √©xito: 100.0%

2. PRUEBA DE MANEJO DE COLISIONES:
   Slot 1234: 12 elementos en cadena
     - government
     - democracy
     - information
     - medical
   
3. PRUEBA DE ELIMINACI√ìN:
   Token a eliminar: 'government'
   Eliminaci√≥n exitosa: True
   Resultado: ‚úì CORRECTO
```

### ‚ö° Rendimiento
- **Tiempo de ejecuci√≥n**: 0.898 segundos
- **Velocidad de b√∫squeda**: 896,985 operaciones/segundo
- **Eficiencia de memoria**: 99.98% de slots utilizados
- **Factor de carga**: 9.083 (alta densidad)

---

## üö´ Actividad 9: Stop List

### üéØ Prop√≥sito
Filtrar palabras comunes e irrelevantes del diccionario para mejorar la calidad del an√°lisis, enfoc√°ndose en t√©rminos discriminativos y sem√°nticamente importantes.

### üîß Funcionamiento T√©cnico

#### Estrategias de Filtrado:

1. **Stop Words Predefinidas** - Listas de palabras comunes (ingl√©s/espa√±ol)
2. **Filtrado por Frecuencia** - Tokens que aparecen en >70% de documentos
3. **Filtrado por Patrones** - N√∫meros, URLs, extensiones, caracteres especiales
4. **Filtrado por Longitud** - Palabras muy cortas (<3 caracteres) o muy largas (>20)

#### Categor√≠as de Stop Words:
```python
predefined_stopwords = {
    # Ingl√©s com√∫n
    'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',
    'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',
    
    # Espa√±ol com√∫n  
    'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se',
    
    # T√©cnicas
    'com', 'org', 'net', 'edu', 'gov', 'www', 'http', 'html'
}
```

#### Algoritmo de Filtrado:
```python
def create_comprehensive_stoplist(self, dictionary_data):
    comprehensive_stoplist = set(predefined_stopwords)
    
    # Agregar basadas en frecuencia (top 100 m√°s comunes)
    frequency_stops = generate_frequency_based_stopwords(dictionary_data, 100)
    comprehensive_stoplist.update(frequency_stops)
    
    # Agregar basadas en patrones
    pattern_stops = generate_pattern_based_stopwords(dictionary_data)
    comprehensive_stoplist.update(pattern_stops)
    
    return comprehensive_stoplist
```

### üìä Impacto del Filtrado

#### Estad√≠sticas de Reducci√≥n:
```
ESTAD√çSTICAS GENERALES:
----------------------------------------
Tokens originales: 90,831
Tokens filtrados: 89,277
Tokens removidos: 1,554
Porcentaje removido: 1.7%

ESTAD√çSTICAS DE FRECUENCIA:
----------------------------------------
Frecuencia total original: 857,723
Frecuencia total filtrada: 569,129
Frecuencia removida: 288,594
Porcentaje de frecuencia removida: 33.6%
```

**An√°lisis**: Aunque solo se removi√≥ el 1.7% de los tokens √∫nicos, se elimin√≥ el 33.6% de la frecuencia total, indicando que se filtraron efectivamente las palabras m√°s comunes e irrelevantes.

### üìÑ Salidas Generadas

#### 1. **stop_words_list.txt** - Lista Completa de Stop Words
```
LISTA DE STOP WORDS - ACTIVIDAD 9
==================================================
Total de stop words: 1,554

PREDEFINED (156 palabras):
the, of, and, to, a, in, that, have, i, it, for, not, on, with...

FREQUENCY (62 palabras):
com, edu, net, org, gov, www, http, html, asp, php...

PATTERN (1,373 palabras):
aa, aaa, 123, 456, &#, &lt, &gt, &amp...

TECHNICAL (43 palabras):
.com, .org, .net, .edu, .gov, .html, .php, .asp...
```

#### 2. **dictionary_filtered.txt** - Diccionario Limpio
```
Token	Repeticiones	#Documentos
--------------------------------------------------
government	459	122
democracy	62	34
information	485	163
medical	284	57
political	156	89
economic	234	67
social	345	123
```

#### 3. **stop_list_analysis.txt** - An√°lisis Comparativo
```
TOP 20 TOKENS REMOVIDOS (M√ÅS FRECUENTES):
----------------------------------------
 1. the: 33,472 veces (395 docs)
 2. of: 21,031 veces (392 docs)
 3. and: 17,252 veces (384 docs)
 4. com: 12,152 veces (188 docs)
 5. to: 11,431 veces (383 docs)

TOP 20 TOKENS RESTANTES (M√ÅS FRECUENTES):
----------------------------------------
 1. government: 459 veces (122 docs)
 2. information: 485 veces (163 docs)
 3. medical: 284 veces (57 docs)
 4. political: 156 veces (89 docs)
```

### ‚ö° Rendimiento
- **Tiempo de ejecuci√≥n**: 0.363 segundos
- **Eficiencia de filtrado**: 33.6% de ruido eliminado
- **Calidad mejorada**: Diccionario m√°s relevante para an√°lisis
- **Tokens preservados**: 89,277 t√©rminos significativos

---

## üìä Actividad 10: TF-IDF Weight Tokens

### üéØ Prop√≥sito
Implementar el c√°lculo de pesos TF-IDF (Term Frequency - Inverse Document Frequency) para determinar la relevancia real de cada token, reemplazando las frecuencias simples con medidas de importancia sem√°ntica.

### üîß Funcionamiento T√©cnico

#### F√≥rmulas TF-IDF:

1. **Term Frequency (TF)**:
   ```
   TF(token, documento) = frecuencia_token / total_tokens_documento
   ```

2. **Inverse Document Frequency (IDF)**:
   ```
   IDF(token) = log(total_documentos / documentos_con_token)
   ```

3. **TF-IDF**:
   ```
   TF-IDF(token, documento) = TF(token, documento) √ó IDF(token)
   ```

#### Algoritmo de C√°lculo:
```python
def calculate_tfidf_scores(self):
    # 1. Calcular TF para cada token en cada documento
    for doc_id, token_counts in self.document_tokens.items():
        total_tokens = sum(token_counts.values())
        for token, count in token_counts.items():
            tf_scores[doc_id][token] = count / total_tokens
    
    # 2. Calcular IDF para cada token
    for token in self.filtered_tokens:
        docs_with_token = len(self.token_documents[token])
        idf_scores[token] = math.log(self.total_documents / docs_with_token)
    
    # 3. Combinar TF-IDF
    for doc_id in self.document_tokens.keys():
        for token, tf_score in self.tf_scores[doc_id].items():
            tfidf_scores[doc_id][token] = tf_score * self.idf_scores[token]
```

#### Interpretaci√≥n de Valores:
- **TF Alto**: Token frecuente en un documento espec√≠fico
- **IDF Alto**: Token raro en el corpus (m√°s discriminativo)  
- **TF-IDF Alto**: Token importante para ese documento espec√≠fico

### üìä An√°lisis de Resultados

#### M√©tricas de Procesamiento:
```
Documentos procesados: 506
Tokens √∫nicos analizados: 89,277
C√°lculos TF realizados: 309,380
C√°lculos IDF realizados: 89,277
C√°lculos TF-IDF realizados: 309,380
```

### üìÑ Salidas Generadas

#### 1. **dictionary_tfidf.txt** - Diccionario con Pesos TF-IDF
```
Token	Max_TF-IDF	Avg_TF-IDF	Total_TF-IDF	#Documentos	IDF
--------------------------------------------------------------------------------
jul	0.905801	0.568847	6.257314	11	3.828641
plain	0.885215	0.511285	6.135418	12	3.741630
risks	0.728662	0.213171	5.116111	24	3.048483
text	0.588830	0.117359	4.929073	42	2.488867
robot	0.317380	0.101380	2.635877	26	2.968440
microsoft	1.870815	0.171484	2.057811	12	3.741630
government	0.096567	0.011042	1.435499	130	1.359002
```

**Explicaci√≥n de columnas:**
- **Max_TF-IDF**: Valor m√°ximo de TF-IDF para este token
- **Avg_TF-IDF**: Promedio de TF-IDF en todos sus documentos
- **Total_TF-IDF**: Suma de todos los valores TF-IDF
- **#Documentos**: N√∫mero de documentos que contienen el token
- **IDF**: Valor de Inverse Document Frequency

#### 2. **document_rankings.txt** - Rankings por Token
```
RANKINGS DE DOCUMENTOS POR TOKEN (TF-IDF)
============================================================

TOKEN: government (IDF: 1.3590)
------------------------------------------------------------
Aparece en 130 documentos:
   1. doc045: TF-IDF=0.096567 (TF=0.071034)
   2. doc123: TF-IDF=0.082145 (TF=0.060456)
   3. doc078: TF-IDF=0.075234 (TF=0.055367)
   4. doc234: TF-IDF=0.068123 (TF=0.050134)

TOKEN: democracy (IDF: 3.0485)
------------------------------------------------------------
Aparece en 34 documentos:
   1. doc087: TF-IDF=0.152425 (TF=0.050000)
   2. doc156: TF-IDF=0.121940 (TF=0.040000)
   3. doc289: TF-IDF=0.091455 (TF=0.030000)
```

#### 3. **discriminative_analysis.txt** - An√°lisis de Tokens Discriminativos
```
AN√ÅLISIS DE TOKENS DISCRIMINATIVOS
============================================================

TOP 20 TOKENS POR IDF (M√ÅS RAROS):
--------------------------------------------------
 1. invites: IDF=6.2265 (1 docs, 0.2%)
 2. concordancia: IDF=6.2265 (1 docs, 0.2%)
 3. palomar: IDF=6.2265 (1 docs, 0.2%)
 4. radiosondes: IDF=6.2265 (1 docs, 0.2%)

TOP 20 TOKENS POR TF-IDF M√ÅXIMO:
--------------------------------------------------
 1. frames: Max_TF-IDF=2.308549 (IDF=4.617099)
 2. microsoft: Max_TF-IDF=1.870815 (IDF=3.741630)
 3. powerpoint: Max_TF-IDF=1.006071 (IDF=5.533389)

TOP 20 TOKENS M√ÅS DISCRIMINATIVOS:
--------------------------------------------------
 1. jul: Score=5.834726 (IDF=3.8286, Max_TF-IDF=0.905801)
 2. plain: Score=5.402156 (IDF=3.7416, Max_TF-IDF=0.885215)
 3. risks: Score=4.721389 (IDF=3.0485, Max_TF-IDF=0.728662)
```

#### 4. **tfidf_statistics.txt** - Estad√≠sticas Detalladas
```
ESTAD√çSTICAS DETALLADAS TF-IDF
==================================================

ESTAD√çSTICAS DE DOCUMENTOS:
------------------------------
Total documentos: 506
Tokens promedio por documento: 611.5
Documento m√°s largo: 2,847 tokens
Documento m√°s corto: 45 tokens

ESTAD√çSTICAS DE IDF:
------------------------------
Total tokens con IDF: 89,277
IDF promedio: 2.8456
IDF m√°ximo: 6.2265
IDF m√≠nimo: 1.0849

ESTAD√çSTICAS DE TF-IDF:
------------------------------
Total c√°lculos TF-IDF: 309,380
TF-IDF promedio: 0.003456
TF-IDF m√°ximo: 2.308549
TF-IDF m√≠nimo: 0.000001
```

### üéØ Beneficios del TF-IDF

1. **üîç Identificaci√≥n de Relevancia**: Tokens importantes para documentos espec√≠ficos
2. **üìâ Penalizaci√≥n de T√©rminos Comunes**: Palabras frecuentes reciben menor peso
3. **üìà Promoci√≥n de T√©rminos Espec√≠ficos**: Palabras raras y discriminativas destacan
4. **üé™ Base para Recuperaci√≥n**: Sistema preparado para motores de b√∫squeda
5. **üìä An√°lisis Sem√°ntico**: Comprensi√≥n del contenido real de documentos

### ‚ö° Rendimiento
- **Tiempo de ejecuci√≥n**: 10.531 segundos
- **C√°lculos realizados**: 309,380 TF-IDF
- **Eficiencia**: 29,380 c√°lculos/segundo
- **Memoria utilizada**: Estructuras optimizadas para 89K tokens

---

## üöÄ C√≥mo Ejecutar

### Opci√≥n 1: Launcher Interactivo (Recomendado)
```bash
cd "c:\Users\ricoj\OneDrive\Escritorio\proyING\actv 1"
python launcher.py
```

**Men√∫ del Launcher:**
```
=== Launcher de Actividades HTML Processor ===

üìã FASE 1 & 2: PROCESAMIENTO B√ÅSICO
1. Actividad 4: Consolidaci√≥n de Palabras
2. Actividad 5: Tokenizaci√≥n
3. Actividad 6: An√°lisis de Diccionario

üìã FASE 3: WEIGHT TOKENS
7. Actividad 7: Archivo Posting
8. Actividad 8: Hash Table
9. Actividad 9: Stop List
10. Actividad 10: TF-IDF Weight Tokens

üìã OPCIONES ESPECIALES
4. Ejecutar Fase 1 & 2 (Actividades 4-6)
11. Ejecutar Fase 3 completa (Actividades 7-10)
12. Ejecutar TODAS las actividades (4-10)
0. Salir
```

### Opci√≥n 2: Ejecuci√≥n Individual
```bash
# Actividad 7: Archivo Posting
python src\activities\actividad7_posting_files.py

# Actividad 8: Hash Table  
python src\activities\actividad8_hash_table.py

# Actividad 9: Stop List
python src\activities\actividad9_stop_list.py

# Actividad 10: TF-IDF
python src\activities\actividad10_tfidf_weights.py
```

### Opci√≥n 3: Ejecuci√≥n Secuencial de Fase 3
```bash
# Ejecutar todas las actividades de Fase 3 en orden
python src\activities\actividad7_posting_files.py
python src\activities\actividad8_hash_table.py
python src\activities\actividad9_stop_list.py
python src\activities\actividad10_tfidf_weights.py
```

---

## üìä Resultados y M√©tricas

### Resumen de Archivos Generados

| Actividad | Archivos Principales | Tama√±o Total | Tiempo Ejecuci√≥n |
|-----------|---------------------|--------------|------------------|
| **Actividad 7** | dictionary_posting.txt, posting_list.txt, posting_stats.txt | 13.2 MB | 1.367 seg |
| **Actividad 8** | hash_table_analysis.txt, hash_table_tests.txt | 6.1 KB | 0.898 seg |
| **Actividad 9** | dictionary_filtered.txt, stop_list_analysis.txt | 1.3 MB | 0.363 seg |
| **Actividad 10** | dictionary_tfidf.txt, document_rankings.txt | 4.2 MB | 10.531 seg |
| **TOTAL FASE 3** | 17 archivos | **18.6 MB** | **13.159 seg** |

### Evoluci√≥n del Diccionario

```
üìä TRANSFORMACI√ìN DEL DICCIONARIO:

Actividad 6  ‚Üí  90,831 tokens  (frecuencias simples)
     ‚Üì
Actividad 7  ‚Üí  90,831 tokens  (posting files)
     ‚Üì  
Actividad 9  ‚Üí  89,277 tokens  (filtrado: -1,554 stop words)
     ‚Üì
Actividad 10 ‚Üí  89,277 tokens  (pesos TF-IDF)

REDUCCI√ìN DE RUIDO: 33.6% de frecuencia removida
MEJORA DE CALIDAD: Tokens discriminativos preservados
```

### M√©tricas de Rendimiento por Actividad

#### Actividad 7 - Posting Files:
```
‚ö° Velocidad de procesamiento: 370 archivos/segundo
üìä Tokens √∫nicos indexados: 90,831
üóÇÔ∏è Entradas de posting generadas: 694,819
üíæ Compresi√≥n de datos: Eficiente
```

#### Actividad 8 - Hash Table:
```
‚ö° Velocidad de b√∫squeda: 896,985 b√∫squedas/segundo
üîß Factor de carga: 9.083 (alta densidad)
üèóÔ∏è Manejo de colisiones: Encadenamiento separado
üìà Eficiencia de memoria: 99.98%
```

#### Actividad 9 - Stop List:
```
üö´ Tokens removidos: 1,554 (1.7%)
üî• Frecuencia filtrada: 288,594 (33.6%)
üéØ Efectividad: Alta (ruido vs. se√±al)
üí° Calidad mejorada: T√©rminos m√°s relevantes
```

#### Actividad 10 - TF-IDF:
```
üßÆ C√°lculos TF-IDF: 309,380
üìä Velocidad de c√°lculo: 29,380 ops/segundo
üéØ Documentos analizados: 506
üîç Tokens con pesos: 89,277
```

---

## üèóÔ∏è Arquitectura T√©cnica

### Estructura de Directorios
```
proyecto/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ activities/
‚îÇ       ‚îú‚îÄ‚îÄ actividad7_posting_files.py     # Indexaci√≥n y posting
‚îÇ       ‚îú‚îÄ‚îÄ actividad8_hash_table.py        # Optimizaci√≥n con hash
‚îÇ       ‚îú‚îÄ‚îÄ actividad9_stop_list.py         # Filtrado inteligente
‚îÇ       ‚îî‚îÄ‚îÄ actividad10_tfidf_weights.py    # An√°lisis sem√°ntico
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/Files/                        # 506 archivos HTML
‚îÇ   ‚îî‚îÄ‚îÄ output/
‚îÇ       ‚îú‚îÄ‚îÄ activity7/                      # Posting files
‚îÇ       ‚îú‚îÄ‚îÄ activity8/                      # Hash table analysis
‚îÇ       ‚îú‚îÄ‚îÄ activity9/                      # Filtered dictionary
‚îÇ       ‚îî‚îÄ‚îÄ activity10/                     # TF-IDF weights
‚îî‚îÄ‚îÄ launcher.py                             # Ejecutor principal
```

### Flujo de Datos
```mermaid
graph TD
    A[506 Archivos HTML] --> B[Actividad 7: Posting]
    B --> C[Dictionary + Posting Lists]
    C --> D[Actividad 8: Hash Table]
    D --> E[Optimizaci√≥n de B√∫squedas]
    C --> F[Actividad 9: Stop List]
    F --> G[Dictionary Filtrado]
    G --> H[Actividad 10: TF-IDF]
    H --> I[Pesos Sem√°nticos]
```

### Dependencias T√©cnicas
```python
# Librer√≠as principales utilizadas
import os, sys, time          # Sistema y temporizaci√≥n
import re                     # Expresiones regulares
import math                   # C√°lculos matem√°ticos (log)
import hashlib               # Funciones hash (SHA256)
from pathlib import Path     # Manejo de rutas
from collections import defaultdict, Counter  # Estructuras de datos
from typing import Dict, List, Tuple, Set, Any  # Tipado
```

### Algoritmos Implementados

1. **üìá Indexaci√≥n Inversa** (Actividad 7)
   - Complejidad: O(n √ó m) donde n=docs, m=tokens_promedio
   - Espacio: O(k) donde k=tokens_√∫nicos

2. **üî® Hash Table** (Actividad 8)
   - B√∫squeda: O(1) promedio, O(n) peor caso
   - Inserci√≥n: O(1) promedio
   - Funci√≥n hash: SHA256 para distribuci√≥n uniforme

3. **üö´ Filtrado Multi-criterio** (Actividad 9)
   - Frecuencia: O(n log n) para ordenamiento
   - Patrones: O(n √ó p) donde p=patrones
   - Combinaci√≥n: O(n) para uni√≥n de conjuntos

4. **üìä TF-IDF** (Actividad 10)
   - TF: O(d √ó t) donde d=docs, t=tokens_doc
   - IDF: O(v) donde v=vocabulario
   - TF-IDF: O(d √ó t) para combinaci√≥n

### Optimizaciones Implementadas

1. **Memoria**:
   - Uso de `defaultdict` y `Counter` para eficiencia
   - Procesamiento por lotes para archivos grandes
   - Liberaci√≥n de memoria despu√©s de cada etapa

2. **I/O**:
   - Manejo robusto de encodings (UTF-8, Latin-1, CP1252)
   - Buffer de escritura optimizado
   - Procesamiento streaming para archivos grandes

3. **Algoritmos**:
   - Hash SHA256 para distribuci√≥n uniforme
   - Encadenamiento separado para manejo de colisiones
   - Estructuras de datos especializadas por caso de uso

---

## üéØ Casos de Uso y Aplicaciones

### 1. **üîç Motor de B√∫squeda**
Los archivos posting permiten b√∫squedas r√°pidas:
```python
# Buscar documentos que contienen "government"
posting_list["government"] = [
    ("doc045", 0.096567),  # TF-IDF score
    ("doc123", 0.082145),
    ("doc078", 0.075234)
]
```

### 2. **üìä An√°lisis de Corpus**
TF-IDF identifica t√©rminos caracter√≠sticos:
```python
# T√©rminos m√°s discriminativos
discriminative_terms = [
    ("invites", 6.2265),     # IDF muy alto = muy espec√≠fico
    ("concordancia", 6.2265), # Aparece en 1 solo documento
    ("palomar", 6.2265)      # T√©rmino √∫nico y relevante
]
```

### 3. **üè∑Ô∏è Clasificaci√≥n de Documentos**
Basada en vectores TF-IDF:
```python
document_vector = {
    "government": 0.096567,
    "democracy": 0.152425,
    "political": 0.087234,
    "economic": 0.065478
}
```

### 4. **üìà An√°lisis de Tendencias**
Seguimiento de t√©rminos importantes:
```python
trending_terms = [
    ("covid", 127),      # 127 documentos
    ("vaccine", 89),     # 89 documentos  
    ("pandemic", 156)    # 156 documentos
]
```

---

## üèÜ Conclusiones

### Logros de la Fase 3

1. **üéØ Sistema Completo de Recuperaci√≥n de Informaci√≥n**
   - Indexaci√≥n eficiente con posting files
   - B√∫squedas optimizadas con hash tables
   - Filtrado inteligente de ruido
   - An√°lisis sem√°ntico con TF-IDF

2. **‚ö° Rendimiento Excepcional**
   - 896,985 b√∫squedas por segundo
   - Procesamiento de 506 documentos en ~13 segundos
   - Manejo eficiente de 90K+ tokens √∫nicos

3. **üß† Inteligencia en el Procesamiento**
   - Eliminaci√≥n autom√°tica de stop words
   - Identificaci√≥n de t√©rminos discriminativos
   - C√°lculo de relevancia sem√°ntica real

4. **üìä Calidad de Datos Mejorada**
   - 33.6% de ruido eliminado eficientemente
   - Preservaci√≥n de t√©rminos importantes
   - Pesos TF-IDF m√°s informativos que frecuencias simples

### Aplicabilidad Real

Este sistema est√° listo para uso en:
- **üîç Motores de b√∫squeda** especializados
- **üìö Bibliotecas digitales** y repositorios
- **üè¢ Sistemas corporativos** de gesti√≥n documental
- **üéì Investigaci√≥n acad√©mica** y an√°lisis de literatura
- **üì∞ An√°lisis de contenido** y monitoreo de medios

### Escalabilidad

El dise√±o modular permite:
- **üìà Expansi√≥n a corpus m√°s grandes** con optimizaciones incrementales
- **üîß Integraci√≥n con bases de datos** profesionales
- **üåê Distribuci√≥n en m√∫ltiples servidores** para procesamiento paralelo
- **ü§ñ Integraci√≥n con ML/AI** para an√°lisis avanzados

---

## üìû Soporte y Mantenimiento

### Archivos de Configuraci√≥n
- `src/config/config.py` - Configuraci√≥n general del proyecto
- `src/config/project_config.py` - Configuraci√≥n espec√≠fica de rutas

### Scripts de Utilidad
- `launcher.py` - Ejecutor interactivo principal
- `project_summary.py` - Resumen completo del proyecto
- `show_results.py` - Visualizador de resultados

### Logging y Debugging
Cada actividad genera reportes detallados:
- `a7_matricula.txt` - Reporte de Actividad 7
- `a8_matricula.txt` - Reporte de Actividad 8  
- `a9_matricula.txt` - Reporte de Actividad 9
- `a10_matricula.txt` - Reporte de Actividad 10

---

**üéâ ¬°Proyecto completado exitosamente!**

*Autor: JOSE GPE RICO MORENO*  
*Proyecto: FASIE-1-PROYECTOS-DE-INGENIERIA*  
*Fecha: Octubre 2025*
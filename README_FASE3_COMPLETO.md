# README - Fase 3: Weight Tokens (Actividades 7-10)

## ğŸ“‹ Tabla de Contenidos
- [IntroducciÃ³n](#introducciÃ³n)
- [Actividad 7: Archivo Posting](#actividad-7-archivo-posting)
- [Actividad 8: Hash Table](#actividad-8-hash-table)
- [Actividad 9: Stop List](#actividad-9-stop-list)
- [Actividad 10: TF-IDF Weight Tokens](#actividad-10-tf-idf-weight-tokens)
- [CÃ³mo Ejecutar](#cÃ³mo-ejecutar)
- [Resultados y MÃ©tricas](#resultados-y-mÃ©tricas)
- [Arquitectura TÃ©cnica](#arquitectura-tÃ©cnica)

---

## ğŸ¯ IntroducciÃ³n

La **Fase 3: Weight Tokens** representa la culminaciÃ³n del proyecto de procesamiento HTML, implementando tÃ©cnicas avanzadas de recuperaciÃ³n de informaciÃ³n y anÃ¡lisis de texto. Esta fase transforma el sistema bÃ¡sico de frecuencias en un motor sofisticado de anÃ¡lisis semÃ¡ntico.

### Objetivos de la Fase 3:
1. **ğŸ—‚ï¸ IndexaciÃ³n Avanzada** - Crear estructuras de posting para bÃºsquedas eficientes
2. **âš¡ OptimizaciÃ³n de Rendimiento** - Implementar hash tables para acceso rÃ¡pido
3. **ğŸ§¹ Limpieza Inteligente** - Filtrar tÃ©rminos irrelevantes automÃ¡ticamente
4. **ğŸ“Š AnÃ¡lisis SemÃ¡ntico** - Calcular relevancia real de tÃ©rminos con TF-IDF

### ProgresiÃ³n del Proyecto:
```
Fase 1 (Actividades 1-3) â†’ Procesamiento BÃ¡sico
Fase 2 (Actividades 4-6) â†’ AnÃ¡lisis de Frecuencias  
Fase 3 (Actividades 7-10) â†’ Weight Tokens & SemÃ¡ntica âœ¨
```

---

## ğŸ“‚ Actividad 7: Archivo Posting

### ğŸ¯ PropÃ³sito
Crear archivos posting que permitan identificar rÃ¡pidamente en quÃ© documentos aparece cada token y con quÃ© frecuencia, estableciendo la base para un sistema de recuperaciÃ³n de informaciÃ³n.

### ğŸ”§ Funcionamiento TÃ©cnico

La actividad implementa tres componentes principales:

1. **Diccionario Posting** - Resumen de tokens con estadÃ­sticas
2. **Lista de Posting** - Mapeo detallado token â†’ documentos
3. **AnÃ¡lisis EstadÃ­stico** - MÃ©tricas de distribuciÃ³n

#### Algoritmo de Procesamiento:
```python
for documento in archivos_html:
    1. Limpiar contenido HTML (remover etiquetas)
    2. Extraer tokens alfabÃ©ticos (â‰¥2 caracteres)
    3. Contar frecuencias por documento
    4. Actualizar Ã­ndice inverso: token â†’ {documentos}
    5. Acumular estadÃ­sticas globales
```

### ğŸ“Š Estructuras de Datos

```python
# Estructura principal de posting
token_doc_freq = defaultdict(Counter)  # token â†’ {doc_id: frecuencia}
token_total_freq = Counter()           # token â†’ frecuencia_total

# Ejemplo de datos internos:
# token_doc_freq["government"] = {
#     "doc001": 5,
#     "doc045": 3,
#     "doc123": 8
# }
```

### ğŸ“„ Salidas Generadas

#### 1. **dictionary_posting.txt** - Diccionario con 3 Columnas
```
Token	Repeticiones	#Documentos
--------------------------------------------------
aa	37	8
aaa	24	7
government	459	122
democracy	62	34
information	485	163
medical	284	57
```

**ExplicaciÃ³n de columnas:**
- **Token**: Palabra extraÃ­da del corpus
- **Repeticiones**: Frecuencia total en todos los documentos
- **#Documentos**: NÃºmero de documentos que contienen el token

#### 2. **posting_list.txt** - Lista Detallada de Posting
```
ARCHIVO POSTING - DOCUMENTOS POR TOKEN
============================================================
Total de tokens Ãºnicos: 90,831
Total de documentos: 506
============================================================

TOKEN: government
  Frecuencia total: 459
  Aparece en 122 documentos:
    doc123: 8 veces
    doc045: 7 veces
    doc078: 6 veces
    doc234: 5 veces
    ...

TOKEN: democracy
  Frecuencia total: 62
  Aparece en 34 documentos:
    doc087: 4 veces
    doc156: 3 veces
    doc289: 3 veces
    ...
```

#### 3. **posting_stats.txt** - EstadÃ­sticas Resumidas
```
ESTADÃSTICAS DEL ARCHIVO POSTING
==================================================
Total de tokens Ãºnicos: 90,831
Total de ocurrencias: 857,723
Total de documentos: 506
Promedio de tokens por documento: 1695.1

TOKEN MÃS FRECUENTE:
  'the': 33,472 ocurrencias
  Aparece en 395 documentos

TOKEN MÃS DISTRIBUIDO:
  'the': aparece en 395 documentos
  Frecuencia total: 33,472

TOP 10 TOKENS MÃS FRECUENTES:
   1. the: 33,472 veces (395 docs)
   2. of: 21,031 veces (392 docs)
   3. and: 17,252 veces (384 docs)
   4. com: 12,152 veces (188 docs)
   5. to: 11,431 veces (383 docs)
```

### âš¡ Rendimiento
- **Tiempo de ejecuciÃ³n**: 1.367 segundos
- **Tokens procesados**: 90,831 Ãºnicos
- **Documentos analizados**: 506
- **Archivos generados**: 4 (13.2 MB total)

---

## ğŸ”¨ Actividad 8: Hash Table

### ğŸ¯ PropÃ³sito
Implementar una tabla hash personalizada para optimizar las bÃºsquedas en el diccionario de tokens, proporcionando acceso rÃ¡pido a la informaciÃ³n de cualquier tÃ©rmino.

### ğŸ”§ Funcionamiento TÃ©cnico

#### CaracterÃ­sticas de la ImplementaciÃ³n:

1. **FunciÃ³n Hash SHA256** - DistribuciÃ³n uniforme de elementos
2. **Encadenamiento Separado** - Manejo de colisiones mediante listas enlazadas
3. **AnÃ¡lisis de Rendimiento** - MÃ©tricas de eficiencia en tiempo real
4. **Casos de Prueba** - ValidaciÃ³n automÃ¡tica de funcionalidad

#### Algoritmo de Hash:
```python
def _hash_function(self, key: str) -> int:
    # Usar SHA256 para mejor distribuciÃ³n
    hash_bytes = hashlib.sha256(key.encode('utf-8')).digest()
    # Convertir primeros 4 bytes a entero
    hash_int = int.from_bytes(hash_bytes[:4], byteorder='big')
    return hash_int % self.size
```

#### Manejo de Colisiones:
```python
class HashNode:
    def __init__(self, key: str, value: Dict[str, Any]):
        self.key = key
        self.value = value
        self.next: Optional['HashNode'] = None  # Encadenamiento
```

### ğŸ“Š MÃ©tricas de Rendimiento

#### EstadÃ­sticas de la Tabla Hash:
```
Total de elementos: 90,831
TamaÃ±o de la tabla: 10,000
Slots ocupados: 9,998
Factor de carga: 9.083
Colisiones: 80,833
Longitud promedio de cadena: 9.08
Longitud mÃ¡xima de cadena: 23
```

#### Resultados de Rendimiento:
```
Tiempo de bÃºsqueda: 0.0011 segundos
Velocidad de bÃºsqueda: 896,985 bÃºsquedas/seg
Ratio de encontrados: 1.000 (100%)
Tiempo de inserciÃ³n: 0.0021 segundos
Velocidad de inserciÃ³n: 480,778 inserciones/seg
```

### ğŸ“„ Salidas Generadas

#### 1. **hash_table_analysis.txt** - AnÃ¡lisis Completo
```
ANÃLISIS DE TABLA HASH - ACTIVIDAD 8
============================================================

ESTADÃSTICAS DE LA TABLA HASH:
----------------------------------------
Total de elementos: 90,831
TamaÃ±o de la tabla: 10,000
Factor de carga: 9.083
Colisiones: 80,833

COMPARACIÃ“N DE FUNCIONES HASH:
----------------------------------------
SHA256:
  Colisiones: 85,831
  Factor de carga: 18.166
  DistribuciÃ³n: Excelente

SIMPLE:
  Colisiones: 88,234
  Factor de carga: 18.456
  DistribuciÃ³n: Buena
```

#### 2. **hash_table_tests.txt** - Casos de Prueba
```
CASOS DE PRUEBA - TABLA HASH
==================================================

1. PRUEBA DE INSERCIÃ“N Y BÃšSQUEDA:
   âœ“ 'government': {'repetitions': 459, 'documents': 122}
   âœ“ 'democracy': {'repetitions': 62, 'documents': 34}
   âœ“ 'information': {'repetitions': 485, 'documents': 163}
   
   Resultado: 20/20 pruebas exitosas
   Tasa de Ã©xito: 100.0%

2. PRUEBA DE MANEJO DE COLISIONES:
   Slot 1234: 12 elementos en cadena
     - government
     - democracy
     - information
     - medical
   
3. PRUEBA DE ELIMINACIÃ“N:
   Token a eliminar: 'government'
   EliminaciÃ³n exitosa: True
   Resultado: âœ“ CORRECTO
```

### âš¡ Rendimiento
- **Tiempo de ejecuciÃ³n**: 0.898 segundos
- **Velocidad de bÃºsqueda**: 896,985 operaciones/segundo
- **Eficiencia de memoria**: 99.98% de slots utilizados
- **Factor de carga**: 9.083 (alta densidad)

---

## ğŸš« Actividad 9: Stop List

### ğŸ¯ PropÃ³sito
Filtrar palabras comunes e irrelevantes del diccionario para mejorar la calidad del anÃ¡lisis, enfocÃ¡ndose en tÃ©rminos discriminativos y semÃ¡nticamente importantes.

### ğŸ”§ Funcionamiento TÃ©cnico

#### Estrategias de Filtrado:

1. **Stop Words Predefinidas** - Listas de palabras comunes (inglÃ©s/espaÃ±ol)
2. **Filtrado por Frecuencia** - Tokens que aparecen en >70% de documentos
3. **Filtrado por Patrones** - NÃºmeros, URLs, extensiones, caracteres especiales
4. **Filtrado por Longitud** - Palabras muy cortas (<3 caracteres) o muy largas (>20)

#### CategorÃ­as de Stop Words:
```python
predefined_stopwords = {
    # InglÃ©s comÃºn
    'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',
    'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',
    
    # EspaÃ±ol comÃºn  
    'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se',
    
    # TÃ©cnicas
    'com', 'org', 'net', 'edu', 'gov', 'www', 'http', 'html'
}
```

#### Algoritmo de Filtrado:
```python
def create_comprehensive_stoplist(self, dictionary_data):
    comprehensive_stoplist = set(predefined_stopwords)
    
    # Agregar basadas en frecuencia (top 100 mÃ¡s comunes)
    frequency_stops = generate_frequency_based_stopwords(dictionary_data, 100)
    comprehensive_stoplist.update(frequency_stops)
    
    # Agregar basadas en patrones
    pattern_stops = generate_pattern_based_stopwords(dictionary_data)
    comprehensive_stoplist.update(pattern_stops)
    
    return comprehensive_stoplist
```

### ğŸ“Š Impacto del Filtrado

#### EstadÃ­sticas de ReducciÃ³n:
```
ESTADÃSTICAS GENERALES:
----------------------------------------
Tokens originales: 90,831
Tokens filtrados: 89,277
Tokens removidos: 1,554
Porcentaje removido: 1.7%

ESTADÃSTICAS DE FRECUENCIA:
----------------------------------------
Frecuencia total original: 857,723
Frecuencia total filtrada: 569,129
Frecuencia removida: 288,594
Porcentaje de frecuencia removida: 33.6%
```

**AnÃ¡lisis**: Aunque solo se removiÃ³ el 1.7% de los tokens Ãºnicos, se eliminÃ³ el 33.6% de la frecuencia total, indicando que se filtraron efectivamente las palabras mÃ¡s comunes e irrelevantes.

### ğŸ“„ Salidas Generadas

#### 1. **stop_words_list.txt** - Lista Completa de Stop Words
```
LISTA DE STOP WORDS - ACTIVIDAD 9
==================================================
Total de stop words: 1,554

PREDEFINED (156 palabras):
the, of, and, to, a, in, that, have, i, it, for, not, on, with...

FREQUENCY (62 palabras):
com, edu, net, org, gov, www, http, html, asp, php...

PATTERN (1,373 palabras):
aa, aaa, 123, 456, &#, &lt, &gt, &amp...

TECHNICAL (43 palabras):
.com, .org, .net, .edu, .gov, .html, .php, .asp...
```

#### 2. **dictionary_filtered.txt** - Diccionario Limpio
```
Token	Repeticiones	#Documentos
--------------------------------------------------
government	459	122
democracy	62	34
information	485	163
medical	284	57
political	156	89
economic	234	67
social	345	123
```

#### 3. **stop_list_analysis.txt** - AnÃ¡lisis Comparativo
```
TOP 20 TOKENS REMOVIDOS (MÃS FRECUENTES):
----------------------------------------
 1. the: 33,472 veces (395 docs)
 2. of: 21,031 veces (392 docs)
 3. and: 17,252 veces (384 docs)
 4. com: 12,152 veces (188 docs)
 5. to: 11,431 veces (383 docs)

TOP 20 TOKENS RESTANTES (MÃS FRECUENTES):
----------------------------------------
 1. government: 459 veces (122 docs)
 2. information: 485 veces (163 docs)
 3. medical: 284 veces (57 docs)
 4. political: 156 veces (89 docs)
```

### âš¡ Rendimiento
- **Tiempo de ejecuciÃ³n**: 0.363 segundos
- **Eficiencia de filtrado**: 33.6% de ruido eliminado
- **Calidad mejorada**: Diccionario mÃ¡s relevante para anÃ¡lisis
- **Tokens preservados**: 89,277 tÃ©rminos significativos

---

## ğŸ“Š Actividad 10: TF-IDF Weight Tokens

### ğŸ¯ PropÃ³sito
Implementar el cÃ¡lculo de pesos TF-IDF (Term Frequency - Inverse Document Frequency) para determinar la relevancia real de cada token, reemplazando las frecuencias simples con medidas de importancia semÃ¡ntica.

### ğŸ”§ Funcionamiento TÃ©cnico

#### FÃ³rmulas TF-IDF:

1. **Term Frequency (TF)**:
   ```
   TF(token, documento) = frecuencia_token / total_tokens_documento
   ```

2. **Inverse Document Frequency (IDF)**:
   ```
   IDF(token) = log(total_documentos / documentos_con_token)
   ```

3. **TF-IDF**:
   ```
   TF-IDF(token, documento) = TF(token, documento) Ã— IDF(token)
   ```

#### Algoritmo de CÃ¡lculo:
```python
def calculate_tfidf_scores(self):
    # 1. Calcular TF para cada token en cada documento
    for doc_id, token_counts in self.document_tokens.items():
        total_tokens = sum(token_counts.values())
        for token, count in token_counts.items():
            tf_scores[doc_id][token] = count / total_tokens
    
    # 2. Calcular IDF para cada token
    for token in self.filtered_tokens:
        docs_with_token = len(self.token_documents[token])
        idf_scores[token] = math.log(self.total_documents / docs_with_token)
    
    # 3. Combinar TF-IDF
    for doc_id in self.document_tokens.keys():
        for token, tf_score in self.tf_scores[doc_id].items():
            tfidf_scores[doc_id][token] = tf_score * self.idf_scores[token]
```

#### InterpretaciÃ³n de Valores:
- **TF Alto**: Token frecuente en un documento especÃ­fico
- **IDF Alto**: Token raro en el corpus (mÃ¡s discriminativo)  
- **TF-IDF Alto**: Token importante para ese documento especÃ­fico

### ğŸ“Š AnÃ¡lisis de Resultados

#### MÃ©tricas de Procesamiento:
```
Documentos procesados: 506
Tokens Ãºnicos analizados: 89,277
CÃ¡lculos TF realizados: 309,380
CÃ¡lculos IDF realizados: 89,277
CÃ¡lculos TF-IDF realizados: 309,380
```

### ğŸ“„ Salidas Generadas

#### 1. **dictionary_tfidf.txt** - Diccionario con Pesos TF-IDF
```
Token	Max_TF-IDF	Avg_TF-IDF	Total_TF-IDF	#Documentos	IDF
--------------------------------------------------------------------------------
jul	0.905801	0.568847	6.257314	11	3.828641
plain	0.885215	0.511285	6.135418	12	3.741630
risks	0.728662	0.213171	5.116111	24	3.048483
text	0.588830	0.117359	4.929073	42	2.488867
robot	0.317380	0.101380	2.635877	26	2.968440
microsoft	1.870815	0.171484	2.057811	12	3.741630
government	0.096567	0.011042	1.435499	130	1.359002
```

**ExplicaciÃ³n de columnas:**
- **Max_TF-IDF**: Valor mÃ¡ximo de TF-IDF para este token
- **Avg_TF-IDF**: Promedio de TF-IDF en todos sus documentos
- **Total_TF-IDF**: Suma de todos los valores TF-IDF
- **#Documentos**: NÃºmero de documentos que contienen el token
- **IDF**: Valor de Inverse Document Frequency

#### 2. **document_rankings.txt** - Rankings por Token
```
RANKINGS DE DOCUMENTOS POR TOKEN (TF-IDF)
============================================================

TOKEN: government (IDF: 1.3590)
------------------------------------------------------------
Aparece en 130 documentos:
   1. doc045: TF-IDF=0.096567 (TF=0.071034)
   2. doc123: TF-IDF=0.082145 (TF=0.060456)
   3. doc078: TF-IDF=0.075234 (TF=0.055367)
   4. doc234: TF-IDF=0.068123 (TF=0.050134)

TOKEN: democracy (IDF: 3.0485)
------------------------------------------------------------
Aparece en 34 documentos:
   1. doc087: TF-IDF=0.152425 (TF=0.050000)
   2. doc156: TF-IDF=0.121940 (TF=0.040000)
   3. doc289: TF-IDF=0.091455 (TF=0.030000)
```

#### 3. **discriminative_analysis.txt** - AnÃ¡lisis de Tokens Discriminativos
```
ANÃLISIS DE TOKENS DISCRIMINATIVOS
============================================================

TOP 20 TOKENS POR IDF (MÃS RAROS):
--------------------------------------------------
 1. invites: IDF=6.2265 (1 docs, 0.2%)
 2. concordancia: IDF=6.2265 (1 docs, 0.2%)
 3. palomar: IDF=6.2265 (1 docs, 0.2%)
 4. radiosondes: IDF=6.2265 (1 docs, 0.2%)

TOP 20 TOKENS POR TF-IDF MÃXIMO:
--------------------------------------------------
 1. frames: Max_TF-IDF=2.308549 (IDF=4.617099)
 2. microsoft: Max_TF-IDF=1.870815 (IDF=3.741630)
 3. powerpoint: Max_TF-IDF=1.006071 (IDF=5.533389)

TOP 20 TOKENS MÃS DISCRIMINATIVOS:
--------------------------------------------------
 1. jul: Score=5.834726 (IDF=3.8286, Max_TF-IDF=0.905801)
 2. plain: Score=5.402156 (IDF=3.7416, Max_TF-IDF=0.885215)
 3. risks: Score=4.721389 (IDF=3.0485, Max_TF-IDF=0.728662)
```

#### 4. **tfidf_statistics.txt** - EstadÃ­sticas Detalladas
```
ESTADÃSTICAS DETALLADAS TF-IDF
==================================================

ESTADÃSTICAS DE DOCUMENTOS:
------------------------------
Total documentos: 506
Tokens promedio por documento: 611.5
Documento mÃ¡s largo: 2,847 tokens
Documento mÃ¡s corto: 45 tokens

ESTADÃSTICAS DE IDF:
------------------------------
Total tokens con IDF: 89,277
IDF promedio: 2.8456
IDF mÃ¡ximo: 6.2265
IDF mÃ­nimo: 1.0849

ESTADÃSTICAS DE TF-IDF:
------------------------------
Total cÃ¡lculos TF-IDF: 309,380
TF-IDF promedio: 0.003456
TF-IDF mÃ¡ximo: 2.308549
TF-IDF mÃ­nimo: 0.000001
```

### ğŸ¯ Beneficios del TF-IDF

1. **ğŸ” IdentificaciÃ³n de Relevancia**: Tokens importantes para documentos especÃ­ficos
2. **ğŸ“‰ PenalizaciÃ³n de TÃ©rminos Comunes**: Palabras frecuentes reciben menor peso
3. **ğŸ“ˆ PromociÃ³n de TÃ©rminos EspecÃ­ficos**: Palabras raras y discriminativas destacan
4. **ğŸª Base para RecuperaciÃ³n**: Sistema preparado para motores de bÃºsqueda
5. **ğŸ“Š AnÃ¡lisis SemÃ¡ntico**: ComprensiÃ³n del contenido real de documentos

### âš¡ Rendimiento
- **Tiempo de ejecuciÃ³n**: 10.531 segundos
- **CÃ¡lculos realizados**: 309,380 TF-IDF
- **Eficiencia**: 29,380 cÃ¡lculos/segundo
- **Memoria utilizada**: Estructuras optimizadas para 89K tokens

---

## ğŸš€ CÃ³mo Ejecutar

### OpciÃ³n 1: Launcher Interactivo (Recomendado)
```bash
cd "c:\Users\ricoj\OneDrive\Escritorio\proyING\actv 1"
python launcher.py
```

**MenÃº del Launcher:**
```
=== Launcher de Actividades HTML Processor ===

ğŸ“‹ FASE 1 & 2: PROCESAMIENTO BÃSICO
1. Actividad 4: ConsolidaciÃ³n de Palabras
2. Actividad 5: TokenizaciÃ³n
3. Actividad 6: AnÃ¡lisis de Diccionario

ğŸ“‹ FASE 3: WEIGHT TOKENS
7. Actividad 7: Archivo Posting
8. Actividad 8: Hash Table
9. Actividad 9: Stop List
10. Actividad 10: TF-IDF Weight Tokens

ğŸ“‹ OPCIONES ESPECIALES
4. Ejecutar Fase 1 & 2 (Actividades 4-6)
11. Ejecutar Fase 3 completa (Actividades 7-10)
12. Ejecutar TODAS las actividades (4-10)
0. Salir
```

### OpciÃ³n 2: EjecuciÃ³n Individual
```bash
# Actividad 7: Archivo Posting
python src\activities\actividad7_posting_files.py

# Actividad 8: Hash Table  
python src\activities\actividad8_hash_table.py

# Actividad 9: Stop List
python src\activities\actividad9_stop_list.py

# Actividad 10: TF-IDF
python src\activities\actividad10_tfidf_weights.py
```

### OpciÃ³n 3: EjecuciÃ³n Secuencial de Fase 3
```bash
# Ejecutar todas las actividades de Fase 3 en orden
python src\activities\actividad7_posting_files.py
python src\activities\actividad8_hash_table.py
python src\activities\actividad9_stop_list.py
python src\activities\actividad10_tfidf_weights.py
```

---

## ğŸ“Š Resultados y MÃ©tricas

### Resumen de Archivos Generados

| Actividad | Archivos Principales | TamaÃ±o Total | Tiempo EjecuciÃ³n |
|-----------|---------------------|--------------|------------------|
| **Actividad 7** | dictionary_posting.txt, posting_list.txt, posting_stats.txt | 13.2 MB | 1.367 seg |
| **Actividad 8** | hash_table_analysis.txt, hash_table_tests.txt | 6.1 KB | 0.898 seg |
| **Actividad 9** | dictionary_filtered.txt, stop_list_analysis.txt | 1.3 MB | 0.363 seg |
| **Actividad 10** | dictionary_tfidf.txt, document_rankings.txt | 4.2 MB | 10.531 seg |
| **TOTAL FASE 3** | 17 archivos | **18.6 MB** | **13.159 seg** |

### EvoluciÃ³n del Diccionario

```
ğŸ“Š TRANSFORMACIÃ“N DEL DICCIONARIO:

Actividad 6  â†’  90,831 tokens  (frecuencias simples)
     â†“
Actividad 7  â†’  90,831 tokens  (posting files)
     â†“  
Actividad 9  â†’  89,277 tokens  (filtrado: -1,554 stop words)
     â†“
Actividad 10 â†’  89,277 tokens  (pesos TF-IDF)

REDUCCIÃ“N DE RUIDO: 33.6% de frecuencia removida
MEJORA DE CALIDAD: Tokens discriminativos preservados
```

### MÃ©tricas de Rendimiento por Actividad

#### Actividad 7 - Posting Files:
```
âš¡ Velocidad de procesamiento: 370 archivos/segundo
ğŸ“Š Tokens Ãºnicos indexados: 90,831
ğŸ—‚ï¸ Entradas de posting generadas: 694,819
ğŸ’¾ CompresiÃ³n de datos: Eficiente
```

#### Actividad 8 - Hash Table:
```
âš¡ Velocidad de bÃºsqueda: 896,985 bÃºsquedas/segundo
ğŸ”§ Factor de carga: 9.083 (alta densidad)
ğŸ—ï¸ Manejo de colisiones: Encadenamiento separado
ğŸ“ˆ Eficiencia de memoria: 99.98%
```

#### Actividad 9 - Stop List:
```
ğŸš« Tokens removidos: 1,554 (1.7%)
ğŸ”¥ Frecuencia filtrada: 288,594 (33.6%)
ğŸ¯ Efectividad: Alta (ruido vs. seÃ±al)
ğŸ’¡ Calidad mejorada: TÃ©rminos mÃ¡s relevantes
```

#### Actividad 10 - TF-IDF:
```
ğŸ§® CÃ¡lculos TF-IDF: 309,380
ğŸ“Š Velocidad de cÃ¡lculo: 29,380 ops/segundo
ğŸ¯ Documentos analizados: 506
ğŸ” Tokens con pesos: 89,277
```

---

## ğŸ—ï¸ Arquitectura TÃ©cnica

### Estructura de Directorios
```
proyecto/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ activities/
â”‚       â”œâ”€â”€ actividad7_posting_files.py     # IndexaciÃ³n y posting
â”‚       â”œâ”€â”€ actividad8_hash_table.py        # OptimizaciÃ³n con hash
â”‚       â”œâ”€â”€ actividad9_stop_list.py         # Filtrado inteligente
â”‚       â””â”€â”€ actividad10_tfidf_weights.py    # AnÃ¡lisis semÃ¡ntico
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/Files/                        # 506 archivos HTML
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ activity7/                      # Posting files
â”‚       â”œâ”€â”€ activity8/                      # Hash table analysis
â”‚       â”œâ”€â”€ activity9/                      # Filtered dictionary
â”‚       â””â”€â”€ activity10/                     # TF-IDF weights
â””â”€â”€ launcher.py                             # Ejecutor principal
```

### Flujo de Datos
```mermaid
graph TD
    A[506 Archivos HTML] --> B[Actividad 7: Posting]
    B --> C[Dictionary + Posting Lists]
    C --> D[Actividad 8: Hash Table]
    D --> E[OptimizaciÃ³n de BÃºsquedas]
    C --> F[Actividad 9: Stop List]
    F --> G[Dictionary Filtrado]
    G --> H[Actividad 10: TF-IDF]
    H --> I[Pesos SemÃ¡nticos]
```

### Dependencias TÃ©cnicas
```python
# LibrerÃ­as principales utilizadas
import os, sys, time          # Sistema y temporizaciÃ³n
import re                     # Expresiones regulares
import math                   # CÃ¡lculos matemÃ¡ticos (log)
import hashlib               # Funciones hash (SHA256)
from pathlib import Path     # Manejo de rutas
from collections import defaultdict, Counter  # Estructuras de datos
from typing import Dict, List, Tuple, Set, Any  # Tipado
```

### Algoritmos Implementados

1. **ğŸ“‡ IndexaciÃ³n Inversa** (Actividad 7)
   - Complejidad: O(n Ã— m) donde n=docs, m=tokens_promedio
   - Espacio: O(k) donde k=tokens_Ãºnicos

2. **ğŸ”¨ Hash Table** (Actividad 8)
   - BÃºsqueda: O(1) promedio, O(n) peor caso
   - InserciÃ³n: O(1) promedio
   - FunciÃ³n hash: SHA256 para distribuciÃ³n uniforme

3. **ğŸš« Filtrado Multi-criterio** (Actividad 9)
   - Frecuencia: O(n log n) para ordenamiento
   - Patrones: O(n Ã— p) donde p=patrones
   - CombinaciÃ³n: O(n) para uniÃ³n de conjuntos

4. **ğŸ“Š TF-IDF** (Actividad 10)
   - TF: O(d Ã— t) donde d=docs, t=tokens_doc
   - IDF: O(v) donde v=vocabulario
   - TF-IDF: O(d Ã— t) para combinaciÃ³n

### Optimizaciones Implementadas

1. **Memoria**:
   - Uso de `defaultdict` y `Counter` para eficiencia
   - Procesamiento por lotes para archivos grandes
   - LiberaciÃ³n de memoria despuÃ©s de cada etapa

2. **I/O**:
   - Manejo robusto de encodings (UTF-8, Latin-1, CP1252)
   - Buffer de escritura optimizado
   - Procesamiento streaming para archivos grandes

3. **Algoritmos**:
   - Hash SHA256 para distribuciÃ³n uniforme
   - Encadenamiento separado para manejo de colisiones
   - Estructuras de datos especializadas por caso de uso

---

## ğŸ¯ Casos de Uso y Aplicaciones

### 1. **ğŸ” Motor de BÃºsqueda**
Los archivos posting permiten bÃºsquedas rÃ¡pidas:
```python
# Buscar documentos que contienen "government"
posting_list["government"] = [
    ("doc045", 0.096567),  # TF-IDF score
    ("doc123", 0.082145),
    ("doc078", 0.075234)
]
```

### 2. **ğŸ“Š AnÃ¡lisis de Corpus**
TF-IDF identifica tÃ©rminos caracterÃ­sticos:
```python
# TÃ©rminos mÃ¡s discriminativos
discriminative_terms = [
    ("invites", 6.2265),     # IDF muy alto = muy especÃ­fico
    ("concordancia", 6.2265), # Aparece en 1 solo documento
    ("palomar", 6.2265)      # TÃ©rmino Ãºnico y relevante
]
```

### 3. **ğŸ·ï¸ ClasificaciÃ³n de Documentos**
Basada en vectores TF-IDF:
```python
document_vector = {
    "government": 0.096567,
    "democracy": 0.152425,
    "political": 0.087234,
    "economic": 0.065478
}
```

### 4. **ğŸ“ˆ AnÃ¡lisis de Tendencias**
Seguimiento de tÃ©rminos importantes:
```python
trending_terms = [
    ("covid", 127),      # 127 documentos
    ("vaccine", 89),     # 89 documentos  
    ("pandemic", 156)    # 156 documentos
]
```

---

## ğŸ† Conclusiones

### Logros de la Fase 3

1. **ğŸ¯ Sistema Completo de RecuperaciÃ³n de InformaciÃ³n**
   - IndexaciÃ³n eficiente con posting files
   - BÃºsquedas optimizadas con hash tables
   - Filtrado inteligente de ruido
   - AnÃ¡lisis semÃ¡ntico con TF-IDF

2. **âš¡ Rendimiento Excepcional**
   - 896,985 bÃºsquedas por segundo
   - Procesamiento de 506 documentos en ~13 segundos
   - Manejo eficiente de 90K+ tokens Ãºnicos

3. **ğŸ§  Inteligencia en el Procesamiento**
   - EliminaciÃ³n automÃ¡tica de stop words
   - IdentificaciÃ³n de tÃ©rminos discriminativos
   - CÃ¡lculo de relevancia semÃ¡ntica real

4. **ğŸ“Š Calidad de Datos Mejorada**
   - 33.6% de ruido eliminado eficientemente
   - PreservaciÃ³n de tÃ©rminos importantes
   - Pesos TF-IDF mÃ¡s informativos que frecuencias simples

### Aplicabilidad Real

Este sistema estÃ¡ listo para uso en:
- **ğŸ” Motores de bÃºsqueda** especializados
- **ğŸ“š Bibliotecas digitales** y repositorios
- **ğŸ¢ Sistemas corporativos** de gestiÃ³n documental
- **ğŸ“ InvestigaciÃ³n acadÃ©mica** y anÃ¡lisis de literatura
- **ğŸ“° AnÃ¡lisis de contenido** y monitoreo de medios

### Escalabilidad

El diseÃ±o modular permite:
- **ğŸ“ˆ ExpansiÃ³n a corpus mÃ¡s grandes** con optimizaciones incrementales
- **ğŸ”§ IntegraciÃ³n con bases de datos** profesionales
- **ğŸŒ DistribuciÃ³n en mÃºltiples servidores** para procesamiento paralelo
- **ğŸ¤– IntegraciÃ³n con ML/AI** para anÃ¡lisis avanzados

---

## ğŸ“ Soporte y Mantenimiento

### Archivos de ConfiguraciÃ³n
- `src/config/config.py` - ConfiguraciÃ³n general del proyecto
- `src/config/project_config.py` - ConfiguraciÃ³n especÃ­fica de rutas

### Scripts de Utilidad
- `launcher.py` - Ejecutor interactivo principal
- `project_summary.py` - Resumen completo del proyecto
- `show_results.py` - Visualizador de resultados

### Logging y Debugging
Cada actividad genera reportes detallados:
- `a7_matricula.txt` - Reporte de Actividad 7
- `a8_matricula.txt` - Reporte de Actividad 8  
- `a9_matricula.txt` - Reporte de Actividad 9
- `a10_matricula.txt` - Reporte de Actividad 10

---

**ğŸ‰ Â¡Proyecto completado exitosamente!**

*Autor: JOSE GPE RICO MORENO*  
*Proyecto: FASIE-1-PROYECTOS-DE-INGENIERIA*  
*Fecha: Octubre 2025*